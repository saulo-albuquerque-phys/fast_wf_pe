# -*- coding: utf-8 -*-
"""importing_from_MLGW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSHwPTuod374t_XxWzFI0G4JUXsE6lOh

# INSTALLING LIBRARIES

WHEN RUNNING THIS FIRST LINE, YOU MIGHT NEED TO RESTART THE KERNEL. PLEASE DO RESTART IT AND RUN THIS SAME FIRST LINE AGAIN.
"""

!pip install tensorflow==2.12.0

!pip install keras-tuner

"""# IMPORTING MLGW

WHEN RUNNING THIS FIRST LINE, YOU MIGHT NEED TO RESTART THE KERNEL. PLEASE DO RESTART IT AND RUN THIS SAME FIRST LINE AGAIN.
"""

!git clone https://github.com/stefanoschmidt1995/MLGW.git

cd MLGW

!python setup.py install

!sed -i '37d' mlgw/GW_generator.py

!sed -i '39d' mlgw/GW_generator.py

!sed -i '39d' mlgw/GW_generator.py

import mlgw

import numpy as np

import matplotlib.pyplot as plt

import glob

from mlgw.GW_generator import mode_generator_NN

from mlgw.ML_routines import PCA_model, add_extra_features, jac_extra_features, augment_features

from scipy.special import factorial as fact

"""# IMPORTING AND SAVING MLGW DATA

## DEFINING THE IMPORTING FUNCTIONS
"""

"""
Module NN_model.py
==================

Loads the same Neural Network model to generate the reduced PCA coeffiecients of a WF (this time in jax). This is only related to the final saved model from the vanilla model. If you have trained a new model, consider disregarding this file completely
"""
import sys
import os
import numpy as np
import json
import matplotlib.pyplot as plt
from shutil import copy2
import glob

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from keras_tuner import BayesianOptimization, HyperModel
import tensorflow as tf
from tensorflow import keras
from keras.layers import Dense
from keras.optimizers import Nadam
from keras.callbacks import EarlyStopping, LearningRateScheduler
from keras import backend as backend
import jax
import jax.numpy as jnp
import jax.nn as jnn
from jax import random
from NN_model import mlgw_NN


from pathlib import Path

from NN_model import mlgw_NN

############################################################

class importing_saving_NN_from_mlgw:
  def __init__(self,folder="/content/MLGW/mlgw/TD_models/model_0/22/"):
    self.folder=folder
    self.amplitude_file=folder+"amp_weights_0123.keras"
    self.ph_01=folder+"ph_weights_01.keras"
    self.ph_res_01=folder+"ph_weights_01_residual.keras"
    self.ph_2345=folder+"ph_weights_2345.keras"
    self.ph_residual_coefficients_01=folder+"residual_coefficients_01"
    self.amp_model = mlgw_NN.load_from_file(self.amplitude_file)
    self.ph_01_model = mlgw_NN.load_from_file(self.ph_01)
    self.ph_res_01_model = mlgw_NN.load_from_file(self.ph_res_01)
    self.ph_2345_model = mlgw_NN.load_from_file(self.ph_2345)
    self.ph_residual_coef_01= np.loadtxt(self.ph_residual_coefficients_01)


  def saving_models(self,new_folder_name="new_data/"):
    folder=self.folder
    new_folder_path = os.path.join(folder, new_folder_name)  # Combine folder and new_folder_name
    os.makedirs(new_folder_path, exist_ok=True)
    j=0
    amp_model=self.amp_model
    ph_01_model=self.ph_01_model
    ph_res_01_model=self.ph_res_01_model
    ph_2345_model=self.ph_2345_model
    ph_residual_coefficients_01=self.ph_residual_coef_01
    np.savetxt(new_folder_path+f"ph_residual_coefficients_01.dat",ph_residual_coefficients_01)
    for i in amp_model.layers:
        print(i.name)
        a=i.get_weights()
        print(a)
        np.savetxt(new_folder_path+f"amp_model_weights_layer_{j}.dat",a[0])
        np.savetxt(new_folder_path+f"amp_model_bias_layer_{j}.dat",a[1])
        j+=1
    j=0
    for i in ph_01_model.layers:
        print(i.name)
        a=i.get_weights()
        print(a)
        np.savetxt(new_folder_path+f"ph_model_01_weights_layer_{j}.dat",a[0])
        np.savetxt(new_folder_path+f"ph_model_01_bias_layer_{j}.dat",a[1])
        j+=1
    j=0
    for i in ph_res_01_model.layers:
        print(i.name)
        a=i.get_weights()
        print(a)
        np.savetxt(new_folder_path+f"ph_res_model_01_weights_layer_{j}.dat",a[0])
        np.savetxt(new_folder_path+f"ph_res_model_01_bias_layer_{j}.dat",a[1])
        j+=1
    j=0
    for i in ph_2345_model.layers:
        print(i.name)
        a=i.get_weights()
        print(a)
        np.savetxt(new_folder_path+f"ph_model_2345_weights_layer_{j}.dat",a[0])
        np.savetxt(new_folder_path+f"ph_model_2345_bias_layer_{j}.dat",a[1])
        j+=1

class loading_NN_weights_bias:
  def __init__(self,folder="/content/MLGW/mlgw/TD_models/model_0/22/new_data/", n_amp_layers=2, n_ph_01_layers=3, n_res_ph_01_layers=6, n_ph_2345_layers=2):
    self.importing_folder=folder
    #loading hidden file
    self.residual_coefficients_01=np.loadtxt(self.importing_folder+f"ph_residual_coefficients_01.dat")
    # Initialize w and b as empty lists
    self.w_amp_0 = []
    self.b_amp_0 = []
    for j in range(n_amp_layers):  # or range(number_of_layers) if you have more layers
        self.w_amp_0.append(np.loadtxt(self.importing_folder+f"amp_model_weights_layer_{j}.dat"))
        self.b_amp_0.append(np.loadtxt(self.importing_folder+f"amp_model_bias_layer_{j}.dat"))
    # saving them
    self.w_amp=self.w_amp_0
    self.b_amp=self.b_amp_0
    # Initialize w and b as empty lists
    self.w_ph_01_0 = []
    self.b_ph_01_0 = []
    for j in range(n_ph_01_layers):  # or range(number_of_layers) if you have more layers
        self.w_ph_01_0.append(np.loadtxt(self.importing_folder+f"ph_model_01_weights_layer_{j}.dat"))
        self.b_ph_01_0.append(np.loadtxt(self.importing_folder+f"ph_model_01_bias_layer_{j}.dat"))
    # saving them
    self.w_ph_01=self.w_ph_01_0
    self.b_ph_01=self.b_ph_01_0
    # Initialize w and b as empty lists
    self.w_res_ph_01_0 = []
    self.b_res_ph_01_0 = []
    for j in range(n_res_ph_01_layers):  # or range(number_of_layers) if you have more layers
        self.w_res_ph_01_0.append(np.loadtxt(self.importing_folder+f"ph_res_model_01_weights_layer_{j}.dat"))
        self.b_res_ph_01_0.append(np.loadtxt(self.importing_folder+f"ph_res_model_01_bias_layer_{j}.dat"))
    # saving them
    self.w_res_ph_01=self.w_res_ph_01_0
    self.b_res_ph_01=self.b_res_ph_01_0
    # Initialize w and b as empty lists
    self.w_ph_2345_0 = []
    self.b_ph_2345_0 = []
    for j in range(n_ph_2345_layers):  # or range(number_of_layers) if you have more layers
        self.w_ph_2345_0.append(np.loadtxt(self.importing_folder+f"ph_model_2345_weights_layer_{j}.dat"))
        self.b_ph_2345_0.append(np.loadtxt(self.importing_folder+f"ph_model_2345_bias_layer_{j}.dat"))
    # saving them
    self.w_ph_2345=self.w_ph_2345_0
    self.b_ph_2345=self.b_ph_2345_0

"""## IMPORTING THE MODEL"""

imp=importing_saving_NN_from_mlgw()

imp.saving_models()

loading_NN=loading_NN_weights_bias()